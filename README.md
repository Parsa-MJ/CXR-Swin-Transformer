# Multilabel Chest X-ray Classification Using Swin Transformer & EfficientNet-B0

This repository contains the complete code, processing pipeline, model training scripts, and evaluation tools for **multilabel chest X-ray (CXR) classification** using:

- **Swin Transformer (Swin-Tiny)**
- **EfficientNet-B0**

The goal is to classify five CheXpert findings:

- Cardiomegaly  
- Pneumonia  
- Atelectasis  
- Pleural Effusion  
- No Finding  

The project includes a patient-wise evaluation protocol, statistical significance testing, and interpretability (Grad-CAM).

## ğŸ“‚ Repository Structure

â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py            # Dataset loading & preprocessing
â”‚   â”œâ”€â”€ models.py             # Swin Transformer + EfficientNet models
â”‚   â”œâ”€â”€ train.py              # Training loop, fine-tuning strategy
â”‚   â”œâ”€â”€ evaluate.py           # AUC, bootstrap testing, confusion matrix
â”‚   â”œâ”€â”€ visualize.py          # Saliency maps (Grad-CAM / Grad-CAM++)
â”‚   â””â”€â”€ utils.py              # Helpers for metrics, LR scheduling, etc.
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Swin_Training.ipynb
â”‚   â”œâ”€â”€ EfficientNet_Training.ipynb
â”‚   â””â”€â”€ Inference_and_Visualization.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

## ğŸ“¦ Installation

Install dependencies:

```
pip install -r requirements.txt
```

Or for Colab:

```
!pip install timm einops albumentations pydicom scikit-learn
```

## ğŸ“Š Dataset: CheXpert (Small Subset)

We use **CheXpert-small**, which includes ~2% of the full CheXpert dataset.

**Target labels**
- Cardiomegaly
- Pneumonia
- Atelectasis
- Pleural Effusion
- No Finding

Only **frontal images** are used.  
Uncertain labels (`-1`) are mapped to **0**.

Patient-wise **80/20 split** prevents leakage.

## ğŸ—ï¸ Model Architectures

### Swin Transformer (Swin-Tiny)
- Hierarchical windowed self-attention  
- ImageNet-pretrained  
- Last stage + classifier fine-tuned  

### EfficientNet-B0
- Lightweight, compound scaling  
- Fully fine-tuned  

## ğŸš€ Training Configuration

| Component      | Setting                    |
|----------------|----------------------------|
| Image Size     | 224Ã—224                    |
| Optimizer      | AdamW                      |
| Scheduler      | Cosine Annealing           |
| Loss           | BCE + Focal Loss           |
| Batch Size     | 64                         |
| Epochs         | 10                         |
| Augmentation   | Flip, rotation, jitter, TA |
| Interpretability | Grad-CAM                 |

## ğŸ“ˆ Results Summary

Both models achieved similar performance due to:

- Limited number of epochs (10)
- Moderate dataset size (CheXpert-small)
- Low input resolution (224px)
- Global labels where CNNs perform strongly
